{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1iH-I7gn3q4Whbt14au_-II8-2jNW_YDC","timestamp":1723531769825},{"file_id":"1ctIhBiSrvhzJ9jZa7HILjtSKx6Fn-jQ_","timestamp":1679880007424},{"file_id":"1BQ39-Zg8l6uzcE2TR_Tg4JsJEB2ry_WJ","timestamp":1679877964481},{"file_id":"1Gaahzekj12ZG6dA5CUhKP8qy0ehjHjHm","timestamp":1679555428172}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## https://tinyurl.com/ANLPTutorial3Part3\n","Go to \"File\" -> \"Save a Copy in Drive...\" This lets you create your own copy of the notebook in your Google drive, and any changes you make doesn't impact the shared notebook"],"metadata":{"id":"xzCdSW_qp_6D"}},{"cell_type":"markdown","source":["### Load dataset"],"metadata":{"id":"GbzqCk-TaUpZ"}},{"cell_type":"markdown","source":["We are going to perform sentiment analysis on a popular dataset from Kaggle. We will use three different packages in Python to do this and compare results:\n","1. TextBlob\n","2. VADER\n","3. SentiWordNet"],"metadata":{"id":"vclcDnDIpPMG"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n"],"metadata":{"id":"0ITjhqPdaUDa","executionInfo":{"status":"ok","timestamp":1723531868439,"user_tz":-600,"elapsed":1247,"user":{"displayName":"Brilliant Kiptoo","userId":"14187329443406714582"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","df = pd.read_csv('/content/drive/My Drive/Colab_Notebooks/ANLP/Reddit_Data.csv')\n","df.head(5)"],"metadata":{"id":"qLg1VlByhA8q","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1723531955129,"user_tz":-600,"elapsed":2823,"user":{"displayName":"Brilliant Kiptoo","userId":"14187329443406714582"}},"outputId":"9061e59c-a8d7-45f1-de97-a4d7bcd7a747"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/My Drive/Colab_Notebooks/ANLP/Reddit_Data.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-e3bc2cc611cc>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Colab_Notebooks/ANLP/Reddit_Data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab_Notebooks/ANLP/Reddit_Data.csv'"]}]},{"cell_type":"code","source":["df.shape #dataset contains a total of 37249 rows"],"metadata":{"id":"CYLQhjrSbjWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.category.value_counts() #shows the count of each category"],"metadata":{"id":"WxYcey9ja7vD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","#Visualising sentiment categories\n","labels=['Positive','Neutral','Negative']\n","sns.barplot(x=labels,y=df.category.value_counts())\n","plt.show()"],"metadata":{"id":"uPmTZ7_hkLNH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocessing the data"],"metadata":{"id":"tvWML3irnbqv"}},{"cell_type":"code","source":["df.isna().sum() #Finding empty rows"],"metadata":{"id":"vpUhTZJInelN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.dropna() #Removign empty rows from the datset\n","df.isna().sum()"],"metadata":{"id":"agcbuWVKneid"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","import string\n","import re\n","from nltk.corpus import stopwords\n","\n","from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","\n","lemma_ = nltk.WordNetLemmatizer() #for lemmatization\n","# pstem = nltk.PorterStemmer() #for stemming\n","\n","def cleanData(text):\n","\n","    # To convert the all uppercase to lowercase\n","    text = text.lower()\n","\n","    # This is a reguglar expression to replace anything char that is not alphabet or numeric.\n","    text = re.sub(r\"[^A-Za-z0-9]\",' ', text)\n","\n","    # The above regular expression itself will take care of punctuation, below is an alternative to remove only punctuation.\n","    text = ''.join([char for char in text if char not in string.punctuation])\n","\n","    # Lemmatization\n","    text = [lemma_.lemmatize(word) for word in text.split(' ') if ((word not in stopwords.words('english')) & len(word)!=0)]\n","\n","    # Let's try without stemming\n","    # text = [pstem.stem(word) for word in text.split(' ') if ((word not in stopwords.words('english')) & len(word)!=0)]\n","\n","    return ' '.join(text)"],"metadata":{"id":"ySylttD6negd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Clean the dataset by applying the fuction\n","df['clean_comment'] = df['clean_comment'].apply(cleanData)"],"metadata":{"id":"BzOL1ogAneeX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head(5)"],"metadata":{"id":"9ff0Fqt9necO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sentiment Analysis using TextBlob"],"metadata":{"id":"y4_QudS_p38x"}},{"cell_type":"markdown","source":["TextBlob is a Python library for processing textual data. It provides a consistent API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, and more.\n","\n","The two measures that are used to analyze the sentiment are:\n","\n","* Polarity – talks about how positive or negative the opinion is. Polarity ranges from -1 to 1 (1 is more positive, 0 is neutral, -1 is more negative)\n","* Subjectivity – talks about how subjective the opinion is. Subjectivity ranges from 0 to 1(0 being very objective and 1 being very subjective)\n","\n","We can use TextBlob(text).sentiment to get the Polarity and Subjectivity values."],"metadata":{"id":"saIUYZSpqMvu"}},{"cell_type":"code","source":["from textblob import TextBlob\n","\n","# function to calculate subjectivity\n","def getSubjectivity(sentiment):\n","    return TextBlob(sentiment).sentiment.subjectivity\n","    # function to calculate polarity\n","def getPolarity(sentiment):\n","        return TextBlob(sentiment).sentiment.polarity\n","\n","# function to analyze the sentiment)\n","def analysis(score):\n","    if score < 0:\n","        return -1\n","    elif score == 0:\n","        return 0\n","    else:\n","        return 1"],"metadata":{"id":"dHpyRrhnp8Kq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textBlob = pd.DataFrame(df[['clean_comment','category']])\n","textBlob['Subjectivity'] = textBlob['clean_comment'].apply(getSubjectivity)\n","textBlob['Polarity'] = textBlob['clean_comment'].apply(getPolarity)\n","textBlob['Analysis'] = textBlob['Polarity'].apply(analysis)\n","textBlob.head()"],"metadata":{"id":"eaUc8hKqp8H2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n","\n","print(\"Evaluation metrics:\\n\",classification_report(textBlob['category'],textBlob['Analysis']))\n","print(\"\\nAccuracy is:\\n\",accuracy_score(textBlob['category'],textBlob['Analysis']))\n","print(\"\\nConfusion Matrix:\\n\",confusion_matrix(textBlob['category'],textBlob['Analysis']))"],"metadata":{"id":"YpfRq8d83z5M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(textBlob.category.value_counts())\n","print(textBlob.Analysis.value_counts())"],"metadata":{"id":"menkTAtqp8F0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sentiment Analysis using VADER"],"metadata":{"id":"3PbHwvPetIRu"}},{"cell_type":"markdown","source":["VADER stands for Valence Aware Dictionary and Sentiment Reasoner.  VADER utilizes a mix of lexical highlights (e.g., words) that are, for the most part, marked by their semantic direction as positive or negative. It also takes into account negation in words and can sometimes perform better. VADER gives us a Polarity score and tells us how positive or negative the conclusion is.\n","\n","\n","* The sum of pos, neg, neu intensities gives 1. Compound ranges from -1 to 1 and is the metric used to draw the overall sentiment.\n","> * positive if compound >= 0.5\n","> * neutral if -0.5 < compound < 0.5\n","> * negative if -0.5 >= compound\n","\n"],"metadata":{"id":"zYrJGTFatLbu"}},{"cell_type":"code","source":["!pip install vaderSentiment -q"],"metadata":{"id":"b3ZKpNljYJ1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","analyzer = SentimentIntensityAnalyzer()\n","# function to calculate vader sentiment\n","def vadersentimentanalysis(sentiment):\n","    vs = analyzer.polarity_scores(sentiment)\n","    return vs['compound']\n","\n","# function to analyse\n","def vader_analysis(compound):\n","    if compound >= 0.5:\n","        return 1\n","    elif compound <= -0.5 :\n","        return -1\n","    else:\n","        return 0"],"metadata":{"id":"RgI_-VmttKRg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vadar_ = pd.DataFrame(df[['clean_comment','category']])\n","vadar_['Vader Sentiment'] = vadar_['clean_comment'].apply(vadersentimentanalysis)\n","vadar_['Vader Analysis'] = vadar_['Vader Sentiment'].apply(vader_analysis)\n","vadar_.head()"],"metadata":{"id":"QVEcLowxtKOk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Evaluation metrics:\\n\",classification_report(vadar_['category'],vadar_['Vader Analysis']))\n","print(\"\\nAccuracy is:\\n\",accuracy_score(vadar_['category'],vadar_['Vader Analysis']))\n","print(\"\\nConfusion Matrix:\\n\",confusion_matrix(vadar_['category'],vadar_['Vader Analysis']))"],"metadata":{"id":"MuLiTM9R6CKn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sentiment Analysis using SentiWordNet"],"metadata":{"id":"qPeewg8j-lWC"}},{"cell_type":"markdown","source":["SentiWordNet uses the WordNet database. The first step is to obtain the POS, lemma of each word, which we will then use to obtain the synonym sets (synsets). Positive, negative, objective scores are then calculated for all the possible synsets or the very first synset to label the text.\n","\n","> * if positive score > negative score, the sentiment is positive\n","> * if positive score < negative score, the sentiment is negative\n","> * if positive score = negative score, the sentiment is neutral"],"metadata":{"id":"yXXCdDr6-nUI"}},{"cell_type":"code","source":["nltk.download('wordnet')\n","from nltk.corpus import wordnet\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","\n","pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n","\n","def token_stop_pos(text):\n","    tags = pos_tag(word_tokenize(text))\n","    newlist = []\n","    for word, tag in tags:\n","        if word.lower() not in set(stopwords.words('english')):\n","          newlist.append(tuple([word, pos_dict.get(tag[0])]))\n","    return newlist\n","\n","def lemmatize(pos_data):\n","    lemma_rew = \" \"\n","    for word, pos in pos_data:\n","      if not pos:\n","          lemma = word\n","          lemma_rew = lemma_rew + \" \" + lemma\n","      else:\n","          lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n","          lemma_rew = lemma_rew + \" \" + lemma\n","    return lemma_rew"],"metadata":{"id":"3vxqsnLy_zbx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SentiWordNet_ = pd.DataFrame(df[['clean_comment','category']])\n","SentiWordNet_['POS tagged'] = SentiWordNet_['clean_comment'].apply(token_stop_pos)\n","SentiWordNet_['Lemma'] = SentiWordNet_['POS tagged'].apply(lemmatize)\n","SentiWordNet_.head()"],"metadata":{"id":"_B68CEnjAJwB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('sentiwordnet')\n","from nltk.corpus import sentiwordnet as swn\n","def sentiwordnetanalysis(pos_data):\n","    sentiment = 0\n","    tokens_count = 0\n","    for word, pos in pos_data:\n","        if not pos:\n","            continue\n","        lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n","        if not lemma:\n","            continue\n","        synsets = wordnet.synsets(lemma, pos=pos)\n","        if not synsets:\n","            continue\n","            # Take the first sense, the most common\n","        synset = synsets[0]\n","        swn_synset = swn.senti_synset(synset.name())\n","        sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n","        tokens_count += 1\n","            # print(swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score())\n","        if not tokens_count:\n","            return 0\n","    if sentiment>0:\n","            return int(1)\n","    if sentiment==0:\n","            return int(0)\n","    else:\n","            return int(-1)"],"metadata":{"id":"IWOw92VC-vrJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SentiWordNet_['SWN analysis'] = SentiWordNet_['POS tagged'].apply(sentiwordnetanalysis)\n","SentiWordNet_.head()"],"metadata":{"id":"nVDolv2MCsOw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Evaluation metrics:\\n\",classification_report(SentiWordNet_['category'],SentiWordNet_['SWN analysis']))\n","print(\"\\nAccuracy is:\\n\",accuracy_score(SentiWordNet_['category'],SentiWordNet_['SWN analysis']))\n","print(\"\\nConfusion Matrix:\\n\",confusion_matrix(SentiWordNet_['category'],SentiWordNet_['SWN analysis']))"],"metadata":{"id":"bP7Bt1m9-voi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## That's all. Now try applying these methods to other datasets, and compare the models to examine what works best in those contexts!"],"metadata":{"id":"AAosFjEOo-T0"}}]}